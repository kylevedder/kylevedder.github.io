I believe strongly in [The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html), and I believe our
job as researchers is to find the right tricks, data distributions, and algorithms to scale up deep learning.

I believe one such trick is teaching vision systems to understand motion. My PhD research focused on training
self-supervised models to predict motion via scene flow, and building offline preprocessing pipelines to provide these
motion descriptions without labels.

<!-- I believe the shortest path to getting robust, generally capable robots in the real world is through the construction of [systems whose performance scales with compute and data, *without* requiring human annotations](http://www.incompleteideas.net/IncIdeas/BitterLesson.html).

In service of this, I am interested in designing and scaling fundamentally 3D vision systems that learn just from raw, multi-modal data. My contrarian bet is on the multi-modal and 3D aspects; a high quality, 3D aware representation with diverse data sources should enable more sample efficient and robust downstream policies. Most representations today are 2D for historical reasons (e.g. lots of RGB data, 2D convolutions won the hardware lottery), but I believe this ends up pushing a lot of 3D spacial understand out of the visual representation and into the downstream policy, making them more expensive to learn and less robust.

My current line of work is focused on [tackling scene flow](https://www.argoverse.org/sceneflow), a problem that requires systems to construct a [robust understanding of the dynamics of the 3D world](./zeroflow.html). For data availability reasons, it primarily focuses on the Autonomous Driving domain, but the same principles apply to other domains, e.g. indoor service robots. -->