<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9NWBV84HB2"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-9NWBV84HB2');
</script>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Kyle Vedder's Homepage">
<meta name="author" content="Kyle Vedder">
<link rel="shortcut icon" href="./favicon.ico">
<title>
Kyle Vedder’s Homepage
</title>
<!-- css-->
<link href="./css/style.css" rel="stylesheet">
</head>
<h1 class="centered">
I’m Kyle Vedder
</h1>
<p><img class="centered" src="img/me_outside.jpg" height="400" /></p>
<p>I believe the shortest path to getting robust, generally capable robots in the real world is through the construction of <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">systems whose performance scales with compute and data, <em>without</em> requiring human annotations</a>. The world is fundamentally 3D, but currently most vision systems focus on 2D data simply due to general availability of RGB images and strong hardware acceleration for standard processing methods (e.g. 2D convolutions). I am interested in building such scalable vision systems on top of 3D sensor data (e.g. LiDAR, Stereo) that reasons natively in 3D, in the hope that these 3D representations are more useful for quickly and robustly learning downstream behavioral tasks compared to their 2D counterparts.</p>
<h2 id="background">Background</h2>
<p>I am a CS PhD <a href="img/static/candidate.png">candidate</a> at Penn under <a href="https://www.seas.upenn.edu/~eeaton/">Eric Eaton</a> and <a href="https://www.seas.upenn.edu/~dineshj/">Dinesh Jayaraman</a> in the <a href="https://www.grasp.upenn.edu/">GRASP Lab</a>. Motivated by my goal of developing elder care robots, my research interests lie in the intersection of:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=2Q4LCoJDOyY&amp;t=2284s">Mobile robotics</a></li>
<li><a href="publications/sparse_point_pillars_iros_2022.pdf">Vision-based object detection</a></li>
</ul>
<p>During my undergrad in CS at UMass Amherst I did research under <a href="https://www.joydeepb.com/">Joydeep Biswas</a> in the <a href="https://amrl.cs.umass.edu/">Autonomous Mobile Robotics Lab</a>. My research was in:</p>
<ul>
<li><a href="http://vedder.io/publications/ScaffoldsLaneVedderBiswasPlanRob2017.pdf">Single-Agent Path Finding (SAPF)</a> for sampling based planners</li>
<li><a href="http://vedder.io/publications/expanding_astar_aij.pdf">Anytime Multi-Agent Path Finding (MAPF)</a> for efficient first solution generation</li>
<li><a href="http://vedder.io/publications/MinutebotsRoboCupTDP2017.pdf">Core infrastructure</a> and <a href="http://vedder.io/publications/MinutebotsRoboCupTDP2018.pdf">low level safety system</a> of our <a href="https://amrl.cs.umass.edu/minutebots.html">RoboCup Small Size League team</a></li>
</ul>
<p>I have also done a number of industry internships; I have interned twice at Unidesk (a startup since aquired by Citrix), twice at Google, once at Amazon’s R&amp;D lab, Lab126 (where I worked on their home robot <a href="https://www.aboutamazon.com/news/devices/meet-astro-a-home-robot-unlike-any-other">Astro</a>), and at <a href="https://www.argo.ai/">Argo AI</a> as a Research Intern under <a href="https://faculty.cc.gatech.edu/~hays/">James Hays</a>.</p>
<h2 id="more-information">More Information</h2>
<ul>
<li>Email: kvedder (at) seas.upenn.edu</li>
<li>Resume: <a href="KyleVedderResume.pdf">/resume</a></li>
<li>Publications: <a href="publications.html">/publications</a></li>
<li>GitHub: <a href="https://github.com/kylevedder">kylevedder</a></li>
<li>Twitter: <a href="https://twitter.com/KyleVedder">KyleVedder</a></li>
</ul>
<h2 id="updates">Updates</h2>
<div class="updates">
<ul>
<li>May 18th, 2023: <a href="./zeroflow.html"><em>ZeroFlow: Fast Zero Label Scene Flow via Distillation</em></a> was submitted.</li>
<li>Jan 12th, 2023: <a href="./publications/L2M_eval_preprint.pdf"><em>A Domain-Agnostic Approach for Characterization of Lifelong Learning Systems</em></a> was accepted to Neural Networks.</li>
<li>Jun 30th, 2022: <a href="./sparse_point_pillars.html"><em>Sparse PointPillars</em></a> was accepted to IROS 2022. <a href="./misc/SparsePointPillars_IROS_2022_reviews.txt">(Reviews)</a></li>
<li>Jun 7th, 2022: <a href="https://www.youtube.com/watch?v=JgcR6cFXR5w">Invited talk for <em>Sparse PointPillars</em> at 3D-DLAD</a></li>
<li>Mar 1st, 2022: <a href="./sparse_point_pillars.html">Submitted <em>Sparse PointPillars</em> to IROS 2022</a></li>
<li>Jan 31st, 2022: <a href="./sparse_point_pillars.html"><em>Sparse PointPillars</em></a> was rejected from ICRA 2022. <a href="./misc/SparsePointPillars_ICRA_2022_reviews.pdf">(Reviews)</a></li>
<li>Sep 15th, 2021: <a href="./sparse_point_pillars.html">Submitted <em>Sparse PointPillars</em> to ICRA 2022</a></li>
<li>Jul 20th, 2021: <a href="./xstar.html">Added project webpage for X*</a></li>
<li>Jul 8th, 2021: <a href="misc/SparsePointPillarsSNNPoster.pdf">Poster presented at Sparse Neural Networks on <em>Sparse PointPillars</em></a></li>
<li>Jun 14th, 2021: <a href="publications/sparse_point_pillars_snn_workshop.pdf">Workshop paper accepted as poster to Sparse Neural Networks: <em>Sparse PointPillars: Exploiting Sparsity on Birds-Eye-View Object Detection</em></a></li>
<li>Apr 27th, 2021: <a href="https://www.youtube.com/watch?v=xFFCQVwYeec">My WPEII Presentation: <em>Current Approaches and Future Directions for Point Cloud Object Detection in Intelligent Agents</em></a></li>
<li>Apr 14th, 2021: <a href="misc/KyleVedderWPEII2021.pdf">My WPEII Document: <em>Current Approaches and Future Directions for Point Cloud Object Detection in Intelligent Agents</em></a></li>
<li>Feb 11th, 2021: <a href="misc/mujoco_py.html">Blog post: Setting up <code>mujoco-py</code> for use with on-screen and off-screen rendering</a></li>
<li>Nov 4th, 2020: <a href="http://vedder.io/publications/expanding_astar_aij.pdf">Journal paper accepted to Artificial Intelligence: <em>X*: Anytime Multi-Agent Path Finding for Sparse Domains using Window-Based Iterative Repairs</em></a></li>
<li>Jul 23rd, 2020: <a href="https://www.youtube.com/watch?v=4RkhsIz14Yc">Presentation: <em>From Shapley Values to Explainable AI</em></a></li>
<li>Jun 29rd, 2020: <a href="https://www.youtube.com/watch?v=o7WW2cu1h7c">Demo: <em>Penn Service Robots navigating around Levine</em></a></li>
<li>May 8th, 2020: <a href="misc/shap_for_classification.pdf">Term paper: <em>An Overview of SHAP-based Feature Importance Measures and Their Applications To Classification</em></a>
</div></li>
</ul>
