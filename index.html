<!DOCTYPE html><html><head>
<meta charset="utf-8">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9NWBV84HB2"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-9NWBV84HB2');
</script>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Kyle Vedder's Homepage">
<meta property="og:title" content="Kyle Vedder's Homepage">
<meta property="og:description" content="Kyle Vedder's Homepage">
<meta property="og:image" content="img/me_computer.jpg">
<meta property="twitter:title" content="Kyle Vedder's Homepage">
<meta property="twitter:description" content="Kyle Vedder's Homepage">
<meta property="twitter:image" content="img/me_computer.jpg">
<meta property="og:type" content="website">
<meta name="author" content="Kyle Vedder">
<link rel="shortcut icon" href="./favicon.ico">
<title>
Kyle Vedder’s Homepage
</title>
<!-- css -->
<link href="./css/style.css" rel="stylesheet"> <!-- JavaScript -->
<script type="text/javascript" src="./js/utils.js"></script>
</head>
<body><h1 class="centered">
I’m Kyle Vedder
</h1>
<p><img class="centered" src="img/me_computer.jpg" height="400" /></p>
<p>I believe the shortest path to getting robust, generally capable
robots in the real world is through the construction of <a
href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">systems
whose performance scales with compute and data, <em>without</em>
requiring human annotations</a>.</p>
<p>In service of this, I am interested in designing and scaling
fundamentally 3D vision systems that learn just from raw, multi-modal
data. My contrarian bet is on the multi-modal and 3D aspects; a high
quality, 3D aware representation with diverse data sources should enable
more sample efficient and robust downstream policies. Most
representations today are 2D for historical reasons (e.g. lots of RGB
data, 2D convolutions won the hardware lottery), but I believe this ends
up pushing a lot of 3D spacial understand out of the visual
representation and into the downstream policy, making them more
expensive to learn and less robust.</p>
<p>My current line of work is focused on <a
href="https://www.argoverse.org/sceneflow">tackling scene flow</a>, a
problem that requires systems to construct a <a
href="./zeroflow.html">robust understanding of the dynamics of the 3D
world</a>. For data availability reasons, it primarily focuses on the
Autonomous Driving domain, but the same principles apply to other
domains, e.g. indoor service robots.</p>
<h2 id="background">Background</h2>
<p>I am a CS PhD <a href="img/static/candidate.png">candidate</a> at
Penn under <a href="https://www.seas.upenn.edu/~eeaton/">Eric Eaton</a>
and <a href="https://www.seas.upenn.edu/~dineshj/">Dinesh Jayaraman</a>
in the <a href="https://www.grasp.upenn.edu/">GRASP Lab</a>. My current
line of work is focused on <a
href="https://www.argoverse.org/sceneflow">scene flow</a> with the
general goals of:</p>
<ul>
<li><a href="./zeroflow.html">Finding label-free, scalable 3D scene
understanding formulations</a></li>
<li><a href="./sparse_point_pillars.html">Designing efficient 3D
perception systems</a></li>
</ul>
<p>During my undergrad in CS at UMass Amherst I did research under <a
href="https://www.joydeepb.com/">Joydeep Biswas</a> in the <a
href="https://amrl.cs.umass.edu/">Autonomous Mobile Robotics Lab</a>. My
research was in:</p>
<ul>
<li><a
href="http://vedder.io/publications/ScaffoldsLaneVedderBiswasPlanRob2017.pdf">Single-Agent
Path Finding (SAPF)</a> for sampling based planners</li>
<li><a href="./xstar.html">Anytime Multi-Agent Path Finding (MAPF)</a>
for efficient first solution generation</li>
<li><a
href="http://vedder.io/publications/MinutebotsRoboCupTDP2017.pdf">Core
infrastructure</a> and <a
href="http://vedder.io/publications/MinutebotsRoboCupTDP2018.pdf">low
level safety system</a> of our <a
href="https://amrl.cs.umass.edu/minutebots.html">RoboCup Small Size
League team</a></li>
</ul>
<p>I have also done many industry internships:</p>
<ul>
<li>NVIDIA as a research intern under <a
href="https://chrisding.github.io/">Zhiding Yu</a></li>
<li>ArgoAI as a research intern under <a
href="https://faculty.cc.gatech.edu/~hays/">James Hays</a></li>
<li>Amazon Lab126 on their home robot, <a
href="https://www.aboutamazon.com/news/devices/meet-astro-a-home-robot-unlike-any-other">Astro</a></li>
<li>Google on adwords and ads quality</li>
<li>Unidesk (a startup since aquired by Citrix)</li>
</ul>
<h2 id="more-information">More Information</h2>
<ul>
<li>Email: kvedder (at) seas.upenn.edu</li>
<li>Resume: <a href="KyleVedderResume.pdf">/resume</a></li>
<li>Publications: <a href="publications.html">/publications</a></li>
<li>GitHub: <a href="https://github.com/kylevedder">kylevedder</a></li>
<li>Twitter: <a
href="https://twitter.com/KyleVedder">KyleVedder</a></li>
</ul>
<h2 id="updates">Updates</h2>
<div class="updates">
<ul>
<li>Jul 1st, 2024: <a href="./trackflow.html"><em>I Can’t Believe It’s
Not Scene Flow!</em></a> was accepted to ECCV!</li>
<li>Mar 28th, 2024: <a href="./misc/homogeneous_transforms.html">Blog
post: On Homogeneous Transforms</a></li>
<li>Jan 29th, 2023: Joined Nvidia as a Research Intern!</li>
<li>Jan 16th, 2024: <a href="./zeroflow.html"><em>ZeroFlow: Scalable
Scene Flow via Distillation</em></a> was accepted to ICLR 2024! <a
href="https://openreview.net/forum?id=FRCHDhbxZF">(Reviews)</a></li>
<li>Dec 4th, 2023: <a href="./misc/review_ai_is_good_for_you.html">Book
review: Eric Jang’s book “<em>AI is Good for You</em>”</a></li>
<li>Aug 3rd, 2023: <a
href="https://vedder.io/misc/applying_to_ml_phd.html">Blog post:
Applying to CS PhD programs for Machine Learning: what I wish I
knew</a></li>
<li>Jul 3rd, 2023: <a href="misc/research_dev_env.html">Blog post: My ML
research development environment workflow</a></li>
<li>Jun 18th, 2023: <a href="./zeroflow.html">ZeroFlow</a> was selected
as a highlighted method in the CVPR 2023 <em>Workshop on Autonomous
Driving</em> Scene Flow Challenge!</li>
<li>Jan 12th, 2023: <a href="./publications/L2M_eval_preprint.pdf"><em>A
Domain-Agnostic Approach for Characterization of Lifelong Learning
Systems</em></a> was accepted to Neural Networks.</li>
<li>Jun 30th, 2022: <a href="./sparse_point_pillars.html"><em>Sparse
PointPillars</em></a> was accepted to IROS 2022. <a
href="./misc/SparsePointPillars_IROS_2022_reviews.txt">(Reviews)</a></li>
<li>Jun 7th, 2022: <a
href="https://www.youtube.com/watch?v=JgcR6cFXR5w">Invited talk for
<em>Sparse PointPillars</em> at 3D-DLAD</a></li>
<li>May 15th, 2022: Joined Argo as a Research Intern!</li>
<li>Apr 27th, 2021: Passed my WPEII qual on <em>Current Approaches and
Future Directions for Point Cloud Object Detection in Intelligent
Agents</em> <a href="misc/KyleVedderWPEII2021.pdf">(Document)</a> <a
href="https://www.youtube.com/watch?v=xFFCQVwYeec">(Video)</a></li>
<li>Feb 11th, 2021: <a href="misc/mujoco_py.html">Blog post: Setting up
<code>mujoco-py</code> for use with on-screen and off-screen
rendering</a></li>
<li>Nov 4th, 2020: <a href="./xstar.html"><em>X*: Anytime Multi-Agent
Path Finding for Sparse Domains using Window-Based Iterative
Repairs</em></a> was accepted to Artificial Intelligence Journal!</li>
<li>Jul 23rd, 2020: <a
href="https://www.youtube.com/watch?v=4RkhsIz14Yc">Presentation:
<em>From Shapley Values to Explainable AI</em></a></li>
<li>Jun 29rd, 2020: <a
href="https://www.youtube.com/watch?v=o7WW2cu1h7c">Demo: <em>Penn
Service Robots navigating around Levine</em></a></li>
<li>May 8th, 2020: <a href="misc/shap_for_classification.pdf">Term
paper: <em>An Overview of SHAP-based Feature Importance Measures and
Their Applications To Classification</em></a>
</div></li>
</ul>
</body></html>