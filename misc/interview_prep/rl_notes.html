<head>
<meta charset="utf-8">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9NWBV84HB2"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-9NWBV84HB2');
</script>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="RL Notes">
<meta property="og:title" content="RL Notes">
<meta property="og:description" content="RL Notes">
<meta property="og:image" content="">
<meta property="twitter:title" content="RL Notes">
<meta property="twitter:description" content="RL Notes">
<meta property="twitter:image" content="">
<meta property="og:type" content="website">
<meta name="author" content="Kyle Vedder">
<link rel="shortcut icon" href="../../favicon.ico">
<title>
RL Notes
</title>
<!-- css -->
<link href="../../css/style.css" rel="stylesheet"> <!-- JavaScript -->
<script type="text/javascript" src="../../js/utils.js"></script>
</head>
<h1 id="reinforcement-learning">Reinforcement Learning</h1>
<h2 id="standard-q-learning-discrete-space">Standard Q Learning —
Discrete Space</h2>
<p>Q Learning is an off-policy RL method — the policy that
<em>generates</em> the data does not need to be the same policy that you
are improving using it. In discrete tabular setting, this can be solved
via simple dynamic programming. <img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Ctextstyle%20Q"
alt="Q" title="Q" class="math inline" /> should tell you the discounted
sum of rewards, and thus can be framed recursively as a local
update.</p>
<p>Given state <img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Ctextstyle%20s_t"
alt="s_t" title="s_t" class="math inline" />, action <img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Ctextstyle%20a_t"
alt="a_t" title="a_t" class="math inline" />, reward <img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Ctextstyle%20r_%7Bt%2B1%7D"
alt="r_{t+1}" title="r_{t+1}" class="math inline" /> for taking <img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Ctextstyle%20a_t"
alt="a_t" title="a_t" class="math inline" /> at <img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Ctextstyle%20s_t"
alt="s_t" title="s_t" class="math inline" />, discount factor <img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Ctextstyle%20%5Cgamma"
alt="\gamma" title="\gamma" class="math inline" /></p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20Q_%7B%5Ctextup%7Bnew%7D%7D%28s_t%2C%20a_t%29%20%5Cgets%20%5Cleft%28r_%7Bt%2B1%7D%20%2B%20%5Cgamma%20%5Cmax_%7Ba%27%7D%20Q%28s_%7Bt%2B1%7D%2C%20a%27%29%20%5Cright%29"
alt="Q_{\textup{new}}(s_t, a_t) \gets \left(r_{t+1} + \gamma \max_{a&#39;} Q(s_{t+1}, a&#39;) \right)"
title="Q_{\textup{new}}(s_t, a_t) \gets \left(r_{t+1} + \gamma \max_{a&#39;} Q(s_{t+1}, a&#39;) \right)"
class="math display" /></p>
<p>Thus, written a least squares loss function <img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Ctextstyle%20L_Q"
alt="L_Q" title="L_Q" class="math inline" /></p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20L_Q%28s_t%2C%20a_t%29%20%3D%20%5Cleft%28Q%28s_t%2C%20a_t%29%20-%20%5Cleft%28r_%7Bt%2B1%7D%20%2B%20%5Cgamma%20%5Cmax_%7Ba%27%7D%20Q%28s_%7Bt%2B1%7D%2C%20a%27%29%20%5Cright%29%20%5Cright%29%5E2"
alt="L_Q(s_t, a_t) = \left(Q(s_t, a_t) - \left(r_{t+1} + \gamma \max_{a&#39;} Q(s_{t+1}, a&#39;) \right) \right)^2"
title="L_Q(s_t, a_t) = \left(Q(s_t, a_t) - \left(r_{t+1} + \gamma \max_{a&#39;} Q(s_{t+1}, a&#39;) \right) \right)^2"
class="math display" /></p>
<h2 id="actor-critic-handling-continuious-space">Actor Critic — Handling
Continuious Space</h2>
<p>In discrete space, <img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Ctextstyle%20%5Cmax_%7Ba%27%7D"
alt="\max_{a&#39;}" title="\max_{a&#39;}" class="math inline" /> is
computable as we can enumerate all possible actions. In continuious
action space, this is not possible. Thus, we must replace this
exhaustive max with a learned “actor” that takes actions, with the Q
function taking the role of “critic”.</p>
<h3 id="deep-deterministic-policy-gradient-ddpg">Deep Deterministic
Policy Gradient (DDPG)</h3>
<p>In DDPG, the actor learns a simple determinstic mapping from state
<img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Ctextstyle%20s_t"
alt="s_t" title="s_t" class="math inline" /> to action <img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Ctextstyle%20a_t"
alt="a_t" title="a_t" class="math inline" />, with noise added for
exploration during data collection, i.e.</p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20a_t%20%3D%20%5Cmu_%5Ctheta%28s_t%29%20%2B%5Cmathcal%7BN%7D%280%2C%20%5Csigma%5E2%29"
alt="a_t = \mu_\theta(s_t) +\mathcal{N}(0, \sigma^2)"
title="a_t = \mu_\theta(s_t) +\mathcal{N}(0, \sigma^2)"
class="math display" /></p>
<p>during training and</p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20a_t%20%3D%20%5Cmu_%5Ctheta%28s_t%29"
alt="a_t = \mu_\theta(s_t)" title="a_t = \mu_\theta(s_t)"
class="math display" /></p>
<p>during inference. Thus, for a given batch of data, the critic can be
optimized via a modified <img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Ctextstyle%20Q"
alt="Q" title="Q" class="math inline" /> loss, i.e.</p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20L_Q%28s_t%2C%20a_t%29%20%3D%20%5Cleft%28Q%28s_t%2C%20a_t%29%20-%20%5Cleft%28r_%7Bt%2B1%7D%20%2B%20%5Cgamma%20Q%28s_%7Bt%2B1%7D%2C%20%5Cmu_%5Ctheta%28s_%7Bt%2B1%7D%29%29%20%5Cright%29%20%5Cright%29%5E2"
alt="L_Q(s_t, a_t) = \left(Q(s_t, a_t) - \left(r_{t+1} + \gamma Q(s_{t+1}, \mu_\theta(s_{t+1})) \right) \right)^2"
title="L_Q(s_t, a_t) = \left(Q(s_t, a_t) - \left(r_{t+1} + \gamma Q(s_{t+1}, \mu_\theta(s_{t+1})) \right) \right)^2"
class="math display" /></p>
<p>and then the actor optimized to maximize the <img
style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Ctextstyle%20Q"
alt="Q" title="Q" class="math inline" /> value; this can be done by
simply minimizing the negative of the <img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Ctextstyle%20Q"
alt="Q" title="Q" class="math inline" /> function, i.e.</p>
<p><img style="vertical-align:middle"
src="https://latex.codecogs.com/png.latex?\dpi{200}%5Cdisplaystyle%20L_%5Cmu%28s_t%2C%20%5Ctheta%29%20%3D%20-Q%28s_t%2C%20%5Cmu_%5Ctheta%28s_t%29%29"
alt="L_\mu(s_t, \theta) = -Q(s_t, \mu_\theta(s_t))"
title="L_\mu(s_t, \theta) = -Q(s_t, \mu_\theta(s_t))"
class="math display" /></p>
<!--This actor is often paramaterized as a Gaussian policy --- the network emits an $n$ dimensional mean $\mu$ and variance $\Sigma$ characterizing a distribution actions are then drawn from. While $\Sigma$ could be a full $n \times n$ matrix, it's often diagonal, i.e. each dimension is sampled independently, to form a _diagonal Gaussian policy_.-->
