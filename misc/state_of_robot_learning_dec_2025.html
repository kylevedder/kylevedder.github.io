<!DOCTYPE html><html><head>
<meta charset="utf-8">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9NWBV84HB2"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-9NWBV84HB2');
</script>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="State of Robot Learning, December 2025">
<meta property="og:title" content="State of Robot Learning, December 2025">
<meta property="og:description" content="State of Robot Learning, December 2025">
<meta property="og:image" content="https://vedder.io/img/static/state_of_robot_learning_dec_2025.png">
<meta property="twitter:title" content="State of Robot Learning, December 2025">
<meta property="twitter:description" content="State of Robot Learning, December 2025">
<meta property="twitter:image" content="https://vedder.io/img/static/state_of_robot_learning_dec_2025.png">
<meta property="og:type" content="website">
<meta name="author" content="Kyle Vedder">
<link rel="shortcut icon" href="../favicon.ico">
<title>
State of Robot Learning, December 2025
</title>
<!-- css -->
<link href="../css/style.css" rel="stylesheet"> <!-- JavaScript -->
<script type="text/javascript" src="../js/utils.js"></script>
</head>
<body><h1 id="state-of-robot-learning-december-2025">State of Robot Learning —
December 2025</h1>
<p>Basically all robot learning systems today (December, 2025) are pure
Behavior Cloning (BC, also called Imitation Learning) systems. Humans
provide (near) optimal demonstrations of a task, and machine learning
models try to imitate those actions. Formally, a policy <img
style="vertical-align:middle"
class="latex-math" width="9" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/f3ccfbd5480b00d1bc38012903eb4375.png"
alt="\pi" title="\pi" class="math inline" /> is trained in a supervised
fashion — given the robot’s state <img style="vertical-align:middle"
class="latex-math" width="6" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/cd00e625ad5fa0990831dd09886fe077.png"
alt="s" title="s" class="math inline" /> (i.e. the camera images, robot
joint angles, and maybe task description text), <img
style="vertical-align:middle"
class="latex-math" width="29" height="16" src="../img/compiled/state_of_robot_learning_dec_2025/52d63dd22b5739b208a11f9fb1de9cd6.png"
alt="\pi(s)" title="\pi(s)" class="math inline" /> predicts the
demonstrated actions <img style="vertical-align:middle"
class="latex-math" width="8" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/7fb0675884892e8376989145c015d8ad.png"
alt="a" title="a" class="math inline" /> (often an action
<em>chunk</em>, e.g. the next second of ~50hz).</p>
<p>This doc aims to describe the anatomy of a modern BC stack, as well
as its shortcomings and (incomplete / clunky) workarounds. It then aims
to explain what other approaches people are considering for the future,
and the issues preventing them from being the conventional approach.
Finally, in concludes with some predictions about the future of robot
learning, and navigation advice for the “picks and shovels” salesmen in
the Embodied AI race.</p>
<h2 id="the-anatomy-of-a-2025-robot-learning-stack">The Anatomy of a
2025 Robot Learning Stack</h2>
<h3 id="collecting-human-expert-demonstrations">Collecting human expert
demonstrations</h3>
<p>First and foremost, to do Behavior Cloning you need data to clone.
These come from human demonstrations, and from a variety of sources.</p>
<h4 id="leader-follower-gello-aloha">Leader-Follower (GELLO, ALOHA)</h4>
<p>Humans directly teleoperate a full robot (follower) using a
controller (leader). This can be done with a full copy of the robot
setup (ALOHA<a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a>) or a smaller, lighter scaled down
version (GELLO<a href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"><sup>2</sup></a>).</p>
<p>Pros:</p>
<ul>
<li>Follower robot has the full sensor suite to record all of <img
style="vertical-align:middle"
class="latex-math" width="6" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/cd00e625ad5fa0990831dd09886fe077.png"
alt="s" title="s" class="math inline" /></li>
<li>All demonstrations are kinodynamically feasible, as they were
executed on the robot</li>
</ul>
<p>Cons:</p>
<ul>
<li>Typically <em>much</em> (up to 10x!) slower than humans performing
the task directly with their hands</li>
<li>Operators require multiple weeks of practice to become proficient
enough to make data usable for training</li>
<li>Requires a full robot onsite to collect data — significant
production and capital requirement to scale collection</li>
</ul>
<h4 id="smart-demo-gloves-universal-manipulation-interface">Smart Demo
Gloves (Universal Manipulation Interface)</h4>
<p>Rather than full leader follower, humans hold devices (e.g. Universal
Manipulation Interface<a href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a>) in their hands and use these
devices to perform the task. The end effectors match the robot, along
with a cheap version of the sensor suite onboard the robot to try to
reconstruct <img style="vertical-align:middle"
class="latex-math" width="6" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/cd00e625ad5fa0990831dd09886fe077.png"
alt="s" title="s" class="math inline" />. Devices perform SLAM to get
end effector pose in task space, such that IK can later be used to
estimate full joint state.</p>
<p>Pros:</p>
<ul>
<li>Faster to learn for operators</li>
<li>Faster demonstrations</li>
<li>Cheaper to deploy at scale (e.g. Generalist<a href="#fn4"
class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>,
Sunday<a href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a>)</li>
</ul>
<p>Cons:</p>
<ul>
<li>Noisy reconstruction of <img style="vertical-align:middle"
class="latex-math" width="6" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/cd00e625ad5fa0990831dd09886fe077.png"
alt="s" title="s" class="math inline" /> and <img
style="vertical-align:middle"
class="latex-math" width="8" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/7fb0675884892e8376989145c015d8ad.png"
alt="a" title="a" class="math inline" />, introducing a domain gap that
can severely harm policy performance
<ul>
<li>Proprioception and actions need to be inferred from the SLAM
estimate of end effector pose</li>
<li>Camera images all feature human arms holding a device, but at
inference time the robot sees robot arms instead</li>
</ul></li>
<li>No guarantee of kinodynamic feasibility — human may reach outside of
the workspace as part of the demonstration, or use their arms to achieve
poses that are impossible with the robots</li>
</ul>
<h4 id="direct-human-demonstrations">Direct human demonstrations</h4>
<p>YouTube and other video sources have large scale data of humans
performing all kinds of tasks. Similarly, many factories feature humans
performing dexterous tasks, and these workers can be augmented with
cameras to record their observations, providing an enormous source of
data.</p>
<p>Pros:</p>
<ul>
<li>Easiest data source to acquire</li>
<li>Enormous amounts of diverse data</li>
<li>At full human speed</li>
</ul>
<p>Cons:</p>
<ul>
<li>Enormous gap in reconstructing <img style="vertical-align:middle"
class="latex-math" width="6" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/cd00e625ad5fa0990831dd09886fe077.png"
alt="s" title="s" class="math inline" /> and <img
style="vertical-align:middle"
class="latex-math" width="8" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/7fb0675884892e8376989145c015d8ad.png"
alt="a" title="a" class="math inline" />
<ul>
<li>State may not be from first person view, or be from a different
angle, introducing a large state gap</li>
<li>Actions must be entirely inferred from the raw data, likely via a
pseudolabeling process from another model (e.g. skeleton trackers /
human hand trackers)</li>
</ul></li>
<li>Without full human DoF, trajectories are likely <em>not</em>
kinodynamically feasible, due to torso leaning, shifting weight,
reaching, etc</li>
</ul>
<h3 id="the-hard-problem-of-behavior-cloning-ood-states">The hard
problem of behavior cloning (OOD states)</h3>
<p>Behavior cloning sounds simple in principle — supervise <img
style="vertical-align:middle"
class="latex-math" width="29" height="16" src="../img/compiled/state_of_robot_learning_dec_2025/52d63dd22b5739b208a11f9fb1de9cd6.png"
alt="\pi(s)" title="\pi(s)" class="math inline" /> to predict <img
style="vertical-align:middle"
class="latex-math" width="8" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/7fb0675884892e8376989145c015d8ad.png"
alt="a" title="a" class="math inline" />.</p>
<p>However, even with extremely clean demonstration data these policies
still wander into <em>out of distribution</em> states. There are several
reasons for this:</p>
<ol type="1">
<li>The world will never perfectly match the training data; even at the
same station, minor variations in lighting, background scene, or other
distractors change the information in state <img
style="vertical-align:middle"
class="latex-math" width="6" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/cd00e625ad5fa0990831dd09886fe077.png"
alt="s" title="s" class="math inline" />, which in turn can impact the
prediction of <img style="vertical-align:middle"
class="latex-math" width="8" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/7fb0675884892e8376989145c015d8ad.png"
alt="a" title="a" class="math inline" /></li>
<li>There’s uncertainty inherent in what exactly to do next
(e.g. unfolding a shirt) — both due to inherent partial observability of
<img style="vertical-align:middle"
class="latex-math" width="6" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/cd00e625ad5fa0990831dd09886fe077.png"
alt="s" title="s" class="math inline" /> (e.g. cannot see inside of a
crumpled shirt to see its internal folds) and inherent multi-modality in
the action distribution from demonstrators</li>
<li>Models have prediction error on their actions; because <img
style="vertical-align:middle"
class="latex-math" width="29" height="16" src="../img/compiled/state_of_robot_learning_dec_2025/52d63dd22b5739b208a11f9fb1de9cd6.png"
alt="\pi(s)" title="\pi(s)" class="math inline" /> is making
<em>sequential</em> decisions about <img style="vertical-align:middle"
class="latex-math" width="8" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/7fb0675884892e8376989145c015d8ad.png"
alt="a" title="a" class="math inline" /> that in turn influence the next
state <img style="vertical-align:middle"
class="latex-math" width="10" height="12" src="../img/compiled/state_of_robot_learning_dec_2025/4ae1caa8e948f51339e1ce166b9e52dd.png"
alt="s&#39;" title="s&#39;" class="math inline" />, this error compounds
upon itself as it rolls out recursively</li>
</ol>
<p>Tackling these challenges requires design choices, both for the model
itself and for the data it’s trained on. Modeling choices are important
—strong, data driven priors (e.g. VLMs) and the right model class to
handle the multi-modality in the action distribution (either discrete
autoregression, where the model inherently models the full probability
distribution over the next token, or continuous denoising, where the
model is trained to sample from the true target distribution) — but the
data distribution the model is trained on arguably matters more.</p>
<p>As discussed in 3), naively training these models on expert human
demonstrations will result in the accumulation of errors in their
predictions during inference, leading them to drift out-of-distribution
into states they’ve never seen before. While the strong visual priors of
a VLM can help the model generalize to novel states, there will still be
scenarios where the model fails.</p>
<h3
id="tackling-out-of-distribution-state-performance-by-bringing-them-in-distribution">Tackling
out-of-distribution state performance (by bringing them in
distribution)</h3>
<p>This is why it’s important to not just naively train on expert human
data! In addition to these straightforward task demonstrations, it’s
critical to train the model how to get out of these failure states — a
“DAgger”<a href="#fn6" class="footnote-ref" id="fnref6"
role="doc-noteref"><sup>6</sup></a> style approach. There’s a bit of
nuance to constructing this data — you want to train your model to
<em>leave</em> these bad states, but you do not want to accidentally
train to <em>enter</em> these bad states, lest it imitate this data and
intentionally visit these bad states. Doing this right means carefully
curating your recovery data.</p>
<p>Building out this DAgger data is an iterative process, and an art at
that. You train the model for the given task, observe its failure modes,
concoct a new dataset to try to address those failure modes, retrain,
and retry. This is a tedious process, requiring many hours of very smart
and discerning human time to essentially play whack-a-mole with various
issues. Along the way, you start to develop a touch and feel for the
policy and its issues. Due to the need for rapid iteration, this is
typically done as a post-training step atop a base pretrained policy,
and hopefully that base policy has already seen quite a bit of task data
such that it already mostly knows what it’s doing.</p>
<p>This frustration is compounded by the fact that the touch and feel
you have developed from your task iteration can be completely wiped out
by a new pretraining of the base policy, sometimes presenting a new (but
hopefully much smaller) set of failure modes. This DAgger data can be
included in a pretraining run, and alongside data scale often results in
higher quality predictions and fewer failures. With sufficient effort on
data iteration, policies can be made to be surprisingly robust.</p>
<p>As these policies get more robust, they also take more of your time
to evalaute their performance. If your policy typically fails every 15
seconds, you only need a few minutes of evals comparing training run A
vs B to get signal on their performance. If your policy takes minutes to
hours between failures, you need to spend many hours doing evals to get
any relative signal. It’s tempting to look for offline metrics (e.g. the
validation MSE featured in = Generalist’s blogpost<a href="#fn7"
class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>),
but emperically there is very poor correlation between these offline
metrics and on-robot performance.</p>
<h3 id="speeding-up-your-behavior-cloning-policy-its-hard">Speeding up
your behavior cloning policy (it’s hard!)</h3>
<p>DAgger addresses robustness issues, and avoiding catastrophic
failures can speed up your average time to complete a task, but it does
nothing to improve your speed in best-case-scenario. Given a dataset,
you can discard all but the fastest demonstrations (losing enormous data
scale and likely hurting robustness), or condition on speed (see: Eric
Jang’s “Just Ask For Generalization”<a href="#fn8" class="footnote-ref"
id="fnref8" role="doc-noteref"><sup>8</sup></a>), but none of these
allow for faster than human demonstration performance.</p>
<p>Another trick is to simply execute the policy actions at faster than
realtime (e.g. execute 50hz control at 70hz), but this stresses your low
level control stack and leads to incorrect behavior when interacting
with world physics (e.g. waiting for a garment to settle flat on a table
after being flicked in the air).</p>
<h2 id="beyond-a-behavior-cloning-stack">Beyond a Behavior Cloning
Stack</h2>
<p>The 2025 BC stack kind of sucks. It is not just bottlenecked on data
scale to get generalization, but <em>also</em> the speed of the data
collectors providing the demonstrations and the hustle (and taste) of
the data sommelier doing DAgger to address any failures.</p>
<p>Ideally, we want robot systems that self-improve:</p>
<ul>
<li>they collect their own data to learn and improve from</li>
<li>they may get stuck in bad states, but they can do exploration to
escape, and then automatically learn to avoid that bad state again</li>
<li>they can automatically get faster, becoming super-human at the task
for their embodiment</li>
</ul>
<p>Reinforcement Learning seems to fit this bill. RL has been wildly
successful in the LLM space, and it’s tempting to imagine we can drag
and drop the same techniques into robotics. Unfortunately, this has yet
to pan out, despite several different approaches.</p>
<h3 id="rl-in-llms">RL in LLMs</h3>
<p>LLMs differ from robotics in two important ways:</p>
<ul>
<li>LLMs are able to be rolled out an unlimited number of times from the
identical state <img style="vertical-align:middle"
class="latex-math" width="6" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/cd00e625ad5fa0990831dd09886fe077.png"
alt="s" title="s" class="math inline" /></li>
<li>LLMs start with a very strong base policy</li>
</ul>
<p>Because of these two factors, online, on-policy RL becomes feasible.
Either directly, or after a little bit of supervised fine-tuning from a
few expert demonstrations, the policy can start to achieve a non-zero
success rate from a given state <img style="vertical-align:middle"
class="latex-math" width="6" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/cd00e625ad5fa0990831dd09886fe077.png"
alt="s" title="s" class="math inline" />. This allows for the LLM to
simply be rolled out hundreds or thousands of times from <img
style="vertical-align:middle"
class="latex-math" width="6" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/cd00e625ad5fa0990831dd09886fe077.png"
alt="s" title="s" class="math inline" /> as a form of exploration,
receive (sparse) rewards from the environment on how its performed, and
directly update its policy.</p>
<p>Importantly, this process avoids having to hallucinate a
counterfactual. By rolling out many different trajectories from <img
style="vertical-align:middle"
class="latex-math" width="6" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/cd00e625ad5fa0990831dd09886fe077.png"
alt="s" title="s" class="math inline" />, it avoids having to
hallucinate “what if”s and instead directly receives environment
feedback from its already strong guesses.</p>
<p>Robotics has none of these luxuries in the real world. Given the
state <img style="vertical-align:middle"
class="latex-math" width="6" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/cd00e625ad5fa0990831dd09886fe077.png"
alt="s" title="s" class="math inline" /> of a messy kitchen at the
beginning of a “clean the kitchen” task, we do not have the ability to
easily perfectly replicate the clutter in the kitchen hundreds of times,
nor do we have strong enough base models that we can reliably fully
clean the kitchen with some nonzero success rate.</p>
<p>Thus, we either need to leverage simulation, where we can reliably
reconstruct <img style="vertical-align:middle"
class="latex-math" width="6" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/cd00e625ad5fa0990831dd09886fe077.png"
alt="s" title="s" class="math inline" /> arbitrarily many times (and
suffer the sim to real gap), or we need to be able to hallucinate good
quality answers to counterfactuals given only a single real rollout from
a real state <img style="vertical-align:middle"
class="latex-math" width="6" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/cd00e625ad5fa0990831dd09886fe077.png"
alt="s" title="s" class="math inline" />.</p>
<h3 id="rl-in-sim">RL in Sim</h3>
<p><em>NB: I am not a sim expert.</em></p>
<p>In LLMs, there is no sim-to-real gap — the environments it interacts
with during training are the exact same environments it will see at
inference. However, in robotics, our simulators are a facsimile for the
real world, and often a poor one at that. Simulators have naive physics
models, have to make numerical estimates to handle multiple colliding
bodies, must select contact models with different tradeoffs, are poor
models of non-rigid objects, and have large visual gaps between sim and
real.</p>
<p>For these reasons training policies entirely in simulation performs
very poorly when transferring to the real world. Domain randomization,
i.e. significantly varying the parameters of the simulator, helps, as
does having a highly structured visual input representation (e.g. scan
dots), but outside of locomotion (e.g. RMA<a href="#fn9"
class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>)
this has seen limited success on robots.</p>
<p>There is ongoing work in “world models”, which are effectively
learned simulators. One major reason for hope is, unlike a policy which
needs to know the optimal action given a state, a world model need only
simulate the dynamics given a state and action. In domains with
structure (such as the real world, which has physics composable rules of
interaction), <em>any</em> state action transition data, be it from an
optimal or a random policy, seemingly should aid in learning general
dynamics, hopefully giving us a shot at building a good, general purpose
world model. That said, as of today, I am unaware of any work that comes
close to modeling well the sort of environment interaction dynamics that
we care about for dexterous manipulation.</p>
<h3 id="rl-in-real">RL In Real</h3>
<p>Using real-world data avoids any sim to real gap, the same reason we
were animated to do BC to begin with. However, learning to improve
directly from your own policy rollouts has a number of hurdles.</p>
<p>The goal of an RL improvement loop is to upweight relatively good
actions and downweight relatively bad ones. To know if an action was
<em>relatively</em> good or not, we need to answer counterfactuals; as
we discussed in the LLM section, we don’t have the luxury of simply
running the policy over and over from the same state, trying a bunch of
semi-reasonable actions to estimate the relative performance of action
<img style="vertical-align:middle"
class="latex-math" width="8" height="7" src="../img/compiled/state_of_robot_learning_dec_2025/7fb0675884892e8376989145c015d8ad.png"
alt="a" title="a" class="math inline" /> vs <img
style="vertical-align:middle"
class="latex-math" width="11" height="12" src="../img/compiled/state_of_robot_learning_dec_2025/9f120775680abc4e2d57833984ebd759.png"
alt="a&#39;" title="a&#39;" class="math inline" />. Instead, we need
some sort of system to hallucinate this; either a Q function that
directly estimates discounted reward <img style="vertical-align:middle"
class="latex-math" width="47" height="16" src="../img/compiled/state_of_robot_learning_dec_2025/3de78d2c7a5885b9db3bed911bc4b27a.png"
alt="Q(s, a)" title="Q(s, a)" class="math inline" />, or some knowledge
of the transition dynamics <img style="vertical-align:middle"
class="latex-math" width="80" height="16" src="../img/compiled/state_of_robot_learning_dec_2025/fd9f373e336e9b222073a48fd9da2f09.png"
alt="(s, a) -&gt; s&#39;" title="(s, a) -&gt; s&#39;"
class="math inline" /> and then the Value of nearby state <img
style="vertical-align:middle"
class="latex-math" width="35" height="16" src="../img/compiled/state_of_robot_learning_dec_2025/d8389d514831fc4a9a1bfa3e3c4821b4.png"
alt="V(s&#39;)" title="V(s&#39;)" class="math inline" />.</p>
<p>Notably, both <img style="vertical-align:middle"
class="latex-math" width="12" height="15" src="../img/compiled/state_of_robot_learning_dec_2025/83581cf87e7da3c83aa1a809cf34d817.png"
alt="Q" title="Q" class="math inline" /> and <img
style="vertical-align:middle"
class="latex-math" width="12" height="11" src="../img/compiled/state_of_robot_learning_dec_2025/3328d25cf9bcf6f937ec3f1e737d2af7.png"
alt="V" title="V" class="math inline" /> are a sort of world model by a
different name; rather than predicting some future state in its entirety
as you might imagine out of a learned simulator, its instead baking in a
bunch of long horizon information about how, under good decision making
through future interactions with the world, you will ultimately get to
the goal.</p>
<p>As you might imagine, this too is quite challenging, and learning
good Q or V functions is an open area of research. Very recently,
Physical Intelligence released <img style="vertical-align:middle"
class="latex-math" width="24" height="15" src="../img/compiled/state_of_robot_learning_dec_2025/2210150b8e026d5b1d73149423aa5dad.png"
alt="\pi_{0.6}^*" title="\pi_{0.6}^*" class="math inline" /><a
href="#fn10" class="footnote-ref" id="fnref10"
role="doc-noteref"><sup>10</sup></a>, an approach that performs
advantage weighted regression (BC, but rather than weighting every
transition equally, weight it by <img style="vertical-align:middle"
class="latex-math" width="101" height="16" src="../img/compiled/state_of_robot_learning_dec_2025/586c32e2d9835569f1e04c3b63665be9.png"
alt="Q(s, a) - V(s)" title="Q(s, a) - V(s)" class="math inline" />),
where they show minor improvements beyond that of just doing naive BC on
the same data. However, in many of the tasks, the policy <em>also</em>
required human DAgger data, and it’s clearly not a silver bullet for
real world RL. There is much more work to be done in building good,
reliable Q and V functions such that they work well out of distribution,
without grossly over or under estimating their true values.</p>
<h2 id="predictions-and-advice">Predictions and Advice</h2>
<p>Here’s a bunch of predictions about the future of robot learning:</p>
<ul>
<li>In (at most) 2 years, VLAs (e.g. <img style="vertical-align:middle"
class="latex-math" width="14" height="9" src="../img/compiled/state_of_robot_learning_dec_2025/d18bc56750db0255a607a0af5a49d1dc.png"
alt="\pi_0" title="\pi_0" class="math inline" />) will be replaced by
video model backbones.</li>
<li>In (at most) 10 years, world models are going to work well at
simulating general open world interactions, and we will be doing policy
extraction within them as part of policy training.
<ul>
<li>Traditional sim / video game engines will be data generators for
World Models, but they will be at their core learned end-to-end.</li>
</ul></li>
<li>(Near) expert data collection will still matter for finetuning these
world models</li>
<li>Real world rollouts on real robots will still matter for reaching
superhuman performance on that embodiment</li>
</ul>
<p>As part of understanding where the field is going, many people have
asked me for advice about building “picks and shovels” startups to
profit from the Embodied AGI race. I think:</p>
<ul>
<li>Data labeling is a comodity, and fundamentally a human labor
arbitrage hustle play, not a tech play. You will need to out-operate
Scale AI.</li>
<li>Pretraining data sales is also a hustle play, <em>and</em> requires
making the case that your data is actually helpful to the customer’s
model performance. This is an ops question as well as a technical one,
and we know <em>it’s not simply the case that all robot data
helps</em>.</li>
<li>Evals are a bottleneck, but they are so important to the model
improvement loop that they have to be done in-house. This cannot be
cleaved off and farmed out to a third party.</li>
<li>Data platforms were not one-size-fits-all for Autonomous Vehicles, a
domain where everyone had roughly the same sensors solving the same
problem. There will not be one for for Embodied AGI.</li>
</ul>
<p>I think the only solid foundation for the future is: human
demonstrations will continue to matter. If you build out a hardware plus
software stack for demonstration (either GELLO or UMI) that reduces the
painpoints described above <em>and you can show produces good policies
by training some</em>, you will be an attractive business partner if not
outright acquisition target.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Zhao, T. Z., Kumar, V., Levine, S., &amp; Finn, C.
(2023). Learning Fine-Grained Bimanual Manipulation with Low-Cost
Hardware. Robotics: Science and Systems (RSS).<a href="#fnref1"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Wu, P., Shentu, Y., Yi, Z., Lin, X., &amp; Abbeel, P.
(2023). GELLO: A General, Low-Cost, and Intuitive Teleoperation
Framework for Robot Manipulators. IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS).<a href="#fnref2"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Chi, C., Xu, Z., Pan, C., Cousineau, E., Burchfiel, B.,
Feng, S., Tedrake, R., &amp; Song, S. (2024). Universal Manipulation
Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots.
Robotics: Science and Systems (RSS).<a href="#fnref3"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Generalist AI Team. (2025). GEN-0: Embodied Foundation
Models That Scale with Physical Interaction. Generalist AI Blog.
Available at: https://generalistai.com/blog/nov-04-2025-GEN-0<a
href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Sunday Team. (2025). ACT-1: A Robot Foundation Model
Trained on Zero Robot Data. Sunday AI Journal. Available at:
https://www.sunday.ai/journal/no-robot-data<a href="#fnref5"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Ross, S., Gordon, G., &amp; Bagnell, J. A. (2011). A
Reduction of Imitation Learning and Structured Prediction to No-Regret
Online Learning. AISTATS.<a href="#fnref6" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Generalist AI Team. (2025). GEN-0: Embodied Foundation
Models That Scale with Physical Interaction. Generalist AI Blog.
Available at: https://generalistai.com/blog/nov-04-2025-GEN-0<a
href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Jang, E. (2021). Just Ask for Generalization. [Blog
Post]. Available at: evjang.com/2021/10/23/generalization.html<a
href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Kumar, A., Fu, Z., Pathak, D., &amp; Malik, J. (2021).
RMA: Rapid Motor Adaptation for Legged Robots. Robotics: Science and
Systems (RSS).<a href="#fnref9" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Amin, A., et al. (2025). <img
style="vertical-align:middle"
class="latex-math" width="24" height="15" src="../img/compiled/state_of_robot_learning_dec_2025/82c0455e5102951c81ea02ee2f9cb5b5.png"
alt="\pi^*_{0.6}" title="\pi^*_{0.6}" class="math inline" />: a VLA that
Learns from Experience. Physical Intelligence.<a href="#fnref10"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body></html>