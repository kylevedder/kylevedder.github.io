<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9NWBV84HB2"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-9NWBV84HB2');
</script>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="AV2 2024 Scene Flow Challenge Announcement">
<meta name="author" content="Kyle Vedder">
<link rel="shortcut icon" href="./favicon.ico">
<title>
AV2 2024 Scene Flow Challenge Announcement
</title>
<!-- css -->
<link href="./css/style.css" rel="stylesheet"> <!-- JavaScript -->
<script type="text/javascript" src="./js/utils.js"></script>
</head>
<h1 id="av2-2024-scene-flow-challenge-announcement">AV2 2024 Scene Flow
Challenge Announcement</h1>
<h2 id="tldr">TL;DR</h2>
<p>The AV2 2024 Scene Flow Challenge is focused on the long tail of
scene flow. As part of this year’s challenge, we are announcing a new
scene flow evaluation protocol.</p>
<h2 id="pedestrians-matter-in-scene-flow">Pedestrians matter in scene
flow</h2>
<p>Scene flow estimation is the task of describing the 3D motion field
between temporally successive point clouds. High quality scene flow
methods should be able to describe the 3D motion of any object because
they have an understanding of geometry and motion.</p>
<p>To ensure scene flow estimators live up to this promise, they must be
evaluated across a diversity of objects — it’s just as important to be
able to describe the motion of pedestrians, bicyclists, and
motorcyclists as it is to describe the motion of cars, trucks, and
buses. However, current state-of-the-art scene flow estimators are very
far from robustly describing the motion of small but important
objects.</p>
<h2 id="current-methods-fail-on-non-car-objects">Current methods fail on
non-car objects</h2>
<p>As a qualitative example, consider <a
href="./av_vis/av2/val_75.html#0bae3b5e-417d-3b03-abaa-806b433233b8">Argoverse
2’s val sequence <em>0bae3b5e-417d-3b03-abaa-806b433233b8</em></a>. The
ego vehicle attempts to turn left at an intersection, but pauses to let
two pedestrians cross the street. We select this sequence because the
two pedestrians are moving close to the ego vehicle, providing unusually
high density lidar observations and thus ample point structure for scene
flow methods to detect motion. Somewhat surprisingly, current methods
fail to estimate scene flow for non-car (e.g. pedestrian) objects.</p>
<div class="figure_box">
<p><img src="./img/static/av2_2024_scene_flow/bev.png" class="centered" style="width:50%"></p>
<div class="caption">
<p><em>Figure 1.</em> Accumulated point cloud of AV2’s val
0bae3b5e-417d-3b03-abaa-806b433233b8 sequence. The ego vehicle is
preparing to make a left turn (shown in increasing <span
style="color:red;">red</span> over time) and slows to allow two
pedestrians (shown in <span style="color:blue;">blue</span>) to cross
the street.</p>
</div>
</div>
<div class="figure_box">
<table style="width:100%">
<tr>
<th>
GT
</th>
<th>
NSFP
</th>
<th>
ZeroFlow
</th>
<th>
FastFlow3D
</th>
</tr>
<tr>
<td>
<img src="./img/static/av2_2024_scene_flow/ped_flow_gt.png" alt="GT Image" style="width:100%">
</td>
<td>
<img src="./img/static/av2_2024_scene_flow/ped_flow_nsfp.png" alt="NSFP Image" style="width:100%">
</td>
<td>
<img src="./img/static/av2_2024_scene_flow/ped_flow_zeroflow.png" alt="ZeroFlow Image" style="width:100%">
</td>
<td>
<img src="./img/static/av2_2024_scene_flow/ped_flow_supervised.png" alt="Supervised Image"  style="width:100%">
</td>
</tr>
</table>
<div class="caption">
<p><em>Figure 2.</em> We visualize a cherry picked example of a
pedestrian with unusually high density lidar returns. We expect that
state-of-the-art scene flow methods should work particularly well on
this instance, but find that all methods consistently fail.</p>
<ul>
<li><strong>GT:</strong> Side view of the pedestrians crossing the
street. The ground truth flow vectors, shown in red, depict the flow of
the green points at time t to their positions at t+1 relative to the
blue t+1 point cloud.</li>
<li><strong>Neural Scene Flow Prior (NSFP)</strong> does not estimate
any flow for the pedestrians. NSFP is a popular test-time optimization
method for scene flow due to its simplicity and good Average EPE
performance. Many recent methods use NSFP as their workhorse flow
estimator (Chodosh et al. <span class="citation"
data-cites="Chodosh_2024_WACV">[1]</span>, NSFP++ <span class="citation"
data-cites="Najibi_2022_ECCV">[2]</span>, Vacek et al. <span
class="citation" data-cites="Vacek_2023_arXiv">[3]</span>).</li>
<li><strong>ZeroFlow</strong> <span class="citation"
data-cites="vedder2024zeroflow">[4]</span>, which uses NSFP
pseudo-labels to train a student feed forward network that produces
superior quality flow to its NSFP teacher, also does not capture any of
the pedestrian motion between these two frames.</li>
<li><strong>FastFlow3D</strong>, a fully-supervised scene flow method,
similarly fails to capture meaningful motion.</li>
</ul>
</div>
</div>
<p>Figure 2 qualitatively demonstrates the systematic failures on small
objects seen across various state-of-the-art scene flow methods. These
failures are present in all types of scene flow methods
(e.g. supervised, self-supervised, optimization-based).</p>
<h2
id="current-metrics-are-biased-towards-objects-with-many-points">Current
metrics are biased towards objects with many points</h2>
<p>The most common Scene Flow evaluation metric is Average Endpoint
Error (Average EPE), i.e. the average over the L2 distance between the
endpoint of the estimated vs ground truth flow vector for each point.
Because this evaluation is computed on a per-point basis, not a
per-object basis (pedestrian instances are fairly common, but only
comprise a tiny fraction of the total points), Average EPE is dominated
by background points. On real-world AV datasets, over 80% of lidar
points are from the background (Figure 4).</p>
<p>Chodosh et al. <span class="citation"
data-cites="Chodosh_2024_WACV">[1]</span> present Threeway EPE, an
alternative evaluation metric which partially addresses the foreground /
background imbalance. Specifically, average EPE is computed for three
mutually exclusive buckets: Background Points, Foreground Static (points
moving &lt;0.5m/s), and Foreground Dynamic (points moving &gt;0.5m/s).
As described in their paper, this prevents background points from
dominating the resulting metric; however, it is still biased towards
objects with many points. Foreground Dynamic and Foreground Static are
dominated by points from <code>CAR</code> and
<code>OTHER_VEHICLES</code> (Figure 3), owing to their larger
per-instance object size and thus larger number of point returns.</p>
<div class="figure_box">
<p><img src="./img/static/av2_2024_scene_flow/point_plot.png" class="centered" style="width:70%"></p>
<div class="caption">
<p><em>Figure 3.</em> Plot of number of points from each semantic
meta-class for AV2 val. Although <code>PEDESTRIAN</code> instances are
common, they are a small fraction of the total number of points owing to
their small size relative to <code>CAR</code> and
<code>OTHER_VEHICLES</code>. Number of points (Y axis) shown on a log
scale.</p>
</div>
</div>
<h2 id="our-new-metric-bucketed-scene-flow-eval">Our new metric:
<em>Bucketed Scene Flow Eval</em></h2>
<p>Our evaluation metric, Bucketed Scene Flow Eval, addresses these
issues by breaking down the point distribution across semantic class and
speed in order to directly evaluate performance across the long-tail of
points. Our metric is computed as follows:</p>
<p>First, we assign all points in each frame-pair to a bucket (defined
by the class-speed matrix, e.g. Table 1). We then compute the average
EPE and average ground truth speed per bucket. We compute Normalized EPE
for each of the dynamic buckets (the second speed column onwards is
considered dynamic) by dividing the average EPE by the average
speed.</p>
<div class="figure_box">
<table class="data_table">
<tr>
<th rowspan="2" style="text-align: center;">
Class
</th>
<th colspan="5" style="text-align: center;">
Speed Columns
</th>
</tr>
<tr>
<td>
0-0.4m/s
</td>
<td>
0.4-0.8m/s
</td>
<td>
0.8-1.2m/s
</td>
<td>
…
</td>
<td>
20-∞m/s
</td>
</tr>
<tr>
<td>
<code>BACKGROUND</code>
</td>
<td>
-
</td>
<td>
-
</td>
<td>
-
</td>
<td>
-
</td>
<td>
-
</td>
</tr>
<tr>
<td>
<code>CAR</code>
</td>
<td>
-
</td>
<td>
-
</td>
<td>
-
</td>
<td>
-
</td>
<td>
-
</td>
</tr>
<tr>
<td>
<code>OTHER_VEHICLES</code>
</td>
<td>
-
</td>
<td>
-
</td>
<td>
-
</td>
<td>
-
</td>
<td>
-
</td>
</tr>
<tr>
<td>
<code>PEDESTRIAN</code>
</td>
<td>
-
</td>
<td>
-
</td>
<td>
-
</td>
<td>
-
</td>
<td>
-
</td>
</tr>
<tr>
<td>
<code>ROAD_SIGNS</code>
</td>
<td>
-
</td>
<td>
-
</td>
<td>
-
</td>
<td>
-
</td>
<td>
-
</td>
</tr>
<tr>
<td>
<code>WHEELED_VRU</code>
</td>
<td>
-
</td>
<td>
-
</td>
<td>
-
</td>
<td>
-
</td>
<td>
-
</td>
</tr>
</table>
<div class="caption">
<p><em>Table 1.</em> Example of the structure of the class-speed matrix
for <em>Bucketed Scene Flow Eval</em>.</p>
</div>
</div>
<p>Unlike existing metrics, Normalized EPE allows us to answer the
question: what fraction of the total speed of the point was not
described by the flow vectors in that bucket? A method that only
predicts ego motion (or zero vectors if egomotion is compensated for)
will have 1.0 Normalized EPE for all dynamic buckets, and a method that
perfectly describes all motion will have 0.0 Normalized EPE for all
moving buckets. Methods may achieve errors greater than 1.0 Normalized
EPE by predicting errors with magnitude greater than the average speed;
for example, a method that describes the negative vector of true motion
will get exactly 2.0 Normalized EPE for all moving buckets with points
(the bucket EPE will be exactly 2x the magnitude of the average speed).
The range of Normalized EPE is between 0 (perfect) and ∞ (arbitrarily
bad), and is undefined for buckets without any points.</p>
<p>We summarize the performance of each meta-class with two numbers:</p>
<ol type="1">
<li>We summarize performance on static objects with Average EPE of the
static bucket (the first column). We do not use Normalized EPE, as
dividing by very small or zero ground truth motion is noisy /
undefined.</li>
<li>We summarize performance on dynamic objects with Normalized EPE of
the dynamic buckets. (second speed column onwards).</li>
</ol>
<p>We present the results of the ZeroFlow 3x model at 35m evaluated with
our proposed metric in Table 2.</p>
<div class="figure_box">
<table class="data_table">
<tr>
<th style="text-align: center;">
Class
</th>
<th style="text-align: center;">
Static (Avg EPE)
</th>
<th style="text-align: center;">
Dynamic (Norm EPE)
</th>
</tr>
<tr>
<td>
<code>BACKGROUND</code>
</td>
<td>
0.01
</td>
<td>
-
</td>
</tr>
<tr>
<td>
<code>CAR</code>
</td>
<td>
0.01
</td>
<td>
0.22
</td>
</tr>
<tr>
<td>
<code>OTHER_VEHICLES</code>
</td>
<td>
0.02
</td>
<td>
0.43
</td>
</tr>
<tr>
<td>
<code>PEDESTRIAN</code>
</td>
<td>
0.01
</td>
<td>
0.91
</td>
</tr>
<tr>
<td>
<code>ROAD_SIGNS</code>
</td>
<td>
0.01
</td>
<td>
0.48
</td>
</tr>
<tr>
<td>
<code>WHEELED_VRU</code>
</td>
<td>
0.01
</td>
<td>
0.59
</td>
</tr>
</table>
<div class="caption">
<p><em>Table 2.</em> Example <em>Bucketed Scene Flow Eval</em> results
for the <em>ZeroFlow 3x</em> model from Vedder et al. <span
class="citation" data-cites="vedder2024zeroflow">[4]</span>, limited to
a 35m box around the ego vehicle. <code>BACKGROUND</code> does not
contain any moving points, so it has “–” recorded for its
<em>Dynamic</em> error. <strong>Lower is better</strong>.</p>
</div>
</div>
<p>As we can see in Table 2, ZeroFlow 3x performs well on static points,
and fairly well on <code>CAR</code> (capturing over 75% of motion) but
it performs extremely poorly on <code>PEDESTRIAN</code> (capturing less
than 10% of motion). Thus, <em>Bucketed Scene Flow Eval</em> enables us
to quantitatively measure the qualitative issues we discovered
above.</p>
<p>To summarize overall method performance, we take a mean over the
different meta-classes for each column, giving <code>ZeroFlow 3x</code>
a mean Average Bucketed Error (mABE) of <code>(0.011, 0.526)</code>. In
the context of our challenge, methods are ranked by their mean dynamic
normalized EPE (i.e. second component of mABE).</p>
<div class="figure_box">
<p><img src="./img/static/av2_2024_scene_flow/mean_DNEPE_plot.png" class="centered" style="width:80%"></p>
<div class="caption">
<p><em>Figure 4.</em> Plot of <em>mean Dynamic Normalized EPE</em> for
various configurations of unsupervised ZeroFlow <span class="citation"
data-cites="vedder2024zeroflow">[4]</span> and supervised FastFlow3D
<span class="citation" data-cites="jund2021scalable">[5]</span>.
Supervised methods shown with hatching. <strong>Lower is
better</strong>.</p>
</div>
</div>
<p>Method relative performance ordering is preserved between traditional
metrics like <em>Threeway EPE</em> (e.g. ZeroFlow 3x XL vs ZeroFlow 1x)
and our <em>mean Dynamic Normalized EPE</em>; however, <em>Bucketed
Scene Flow Eval</em> highlights that state-of-the-art scene flow
estimators have enormous room for improvement to make good on the
promise of general motion detection — <em>ZeroFlow 3x XL</em>, which
achieves state-of-the-art on Threeway EPE, on average only describes 50%
of motion per metaclass (Figure 4).</p>
<h2 id="bibliography">Bibliography</h2>
<!-- [@Chodosh_2024_WACV]: Chodosh et al. _Re-Evaluating LiDAR Scene Flow for Autonomous Driving._ WACV 2024. -->
<!-- [@Najibi_2022_ECCV]: Najibi et al. _Motion Inspired Unsupervised Perception and Prediction in Autonomous DrivingMotion Inspired Unsupervised Perception and Prediction in Autonomous Driving._ ECCV 2022.
[@Vacek_2023_arXiv]: Vacek et al. _Regularizing Self-supervised 3D Scene Flows with Surface Awareness and Cyclic Consistency._ arXiv 2023.
[@vedder2024zeroflow]: Vedder et al. _ZeroFlow: Scalable Scene Flow via Distillation._ arXiv 2023.
[@jund2021scalable]: Jund et al. _Scalable Scene Flow from Point Clouds in the Real World._ ICRA 2021. -->
<div id="refs" class="references csl-bib-body" data-entry-spacing="0"
role="list">
<div id="ref-Chodosh_2024_WACV" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">N.
Chodosh, D. Ramanan, and S. Lucey, <span>“Re-evaluating LiDAR scene
flow,”</span> in <em>Proceedings of the IEEE/CVF winter conference on
applications of computer vision (WACV)</em>, 2024, pp. 6005–6015.</div>
</div>
<div id="ref-Najibi_2022_ECCV" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">M.
Najibi <em>et al.</em>, <span>“Motion inspired unsupervised perception
and prediction in autonomous driving,”</span> <em>European Conference on
Computer Vision (ECCV)</em>, 2022.</div>
</div>
<div id="ref-Vacek_2023_arXiv" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">P.
Vacek, D. Hurych, K. Zimmermann, P. Perez, and T. Svoboda,
<span>“Regularizing self-supervised 3D scene flows with surface
awareness and cyclic consistency,”</span> <em>arXiv</em>, 2023.</div>
</div>
<div id="ref-vedder2024zeroflow" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">K.
Vedder <em>et al.</em>, <span>“<span class="nocase">ZeroFlow: Scalable
Scene Flow via Distillation</span>,”</span> in <em>Twelfth International
Conference on Learning Representations (ICLR)</em>, 2024.</div>
</div>
<div id="ref-jund2021scalable" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">P.
Jund, C. Sweeney, N. Abdo, Z. Chen, and J. Shlens, <span>“Scalable scene
flow from point clouds in the real world,”</span> <em>arXiv</em>,
2021.</div>
</div>
</div>
